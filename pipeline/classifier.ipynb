{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997759e3-e618-49cf-89e0-586feddc88b5",
   "metadata": {},
   "source": [
    "### Classifier notebook:\n",
    " - Development of multioutput classification model that uses expected profit per user as metric. \n",
    " - This notebook will be cleaned up and separated into multiple notebooks so that it will be easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef141ef9-6546-4251-8b48-41d0239d8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries, CLEAN THIS \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import multilabel_confusion_matrix, make_scorer\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "sns.set_theme(style='darkgrid')\n",
    "# read in the json files\n",
    "portfolio = pd.read_json('../data/raw/portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json('../data/raw/profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json('../data/raw/transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dda48f-093a-4e1c-a2bf-5c4df7f5381a",
   "metadata": {},
   "source": [
    "### Preparing dataset to be used to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a384ea-b92a-436f-b802-b21b41eddd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get offer ids from 'value' column, convert to float, and store in new column\n",
    "offer_ids = dict()\n",
    "indx = list(transcript[transcript['event']!='transaction'].index)\n",
    "\n",
    "for ind in indx: \n",
    "    offer_id = list(transcript.iloc[ind]['value'].values())[0]\n",
    "    offer_ids.update({ind:offer_id})\n",
    "    \n",
    "# Make dataframe from dictionary of index, offer_id strings     \n",
    "offer_id_df = pd.DataFrame.from_dict(offer_ids, orient='index', columns=['offer_ids'])\n",
    "\n",
    "# Concat transcript_mod and offer_id_df dataframes\n",
    "transcript_mod = pd.concat([transcript, offer_id_df], axis=1, ignore_index=False)\n",
    "\n",
    "# rename column 'id' as offer_ids to remain consistent with transcript df\n",
    "portfolio = portfolio.rename(columns={'id':'offer_ids'})\n",
    "\n",
    "# merge transcript and portfolio dataframes\n",
    "transcript_portfolio = transcript_mod.merge(portfolio[['offer_ids', 'offer_type']], on='offer_ids', how='left')\n",
    "\n",
    "offers = ['bogo', 'discount']\n",
    "# filter transcript_portfolio to get transcripts corresponding to BOGO offers \n",
    "offer = transcript_portfolio[transcript_portfolio['offer_type'].isin(offers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372818d1-02e3-4484-81a5-fb3593b34b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SamaraSadeek/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "offer.drop(['value','time', 'offer_ids'], axis=1, inplace=True)\n",
    "\n",
    "offer_per_person = offer.groupby(['person','offer_type'])['event'].value_counts().unstack()\n",
    "\n",
    "offer_per_person['offer completed'].fillna(0, inplace=True)\n",
    "\n",
    "offer_per_person['completed_per_view'] = offer_per_person['offer completed']/ offer_per_person['offer viewed']\n",
    "\n",
    "offer_per_person['completed_per_view'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f17872-0a6a-4081-a9b9-8157e3958280",
   "metadata": {},
   "source": [
    "### Computing responsiveness label for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "832b74e9-a1c3-4f19-9b81-6f40b553a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setting_response(cpv):\n",
    "    if cpv <= 0.5: \n",
    "        return 0 \n",
    "    else: \n",
    "        return 1\n",
    "    \n",
    "offer_per_person['responds'] = offer_per_person['completed_per_view'].apply(lambda x:setting_response(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1307eb0d-47b4-467d-b0a8-355be5e1fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_df = offer_per_person.unstack()\n",
    "offer_df = offer_df['responds']\n",
    "offer_df.dropna(inplace=True)\n",
    "offer_df = offer_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae531d-9129-4227-bd44-22a065988b9f",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de052c5-b3a6-45bb-8ef5-6f57d58f317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column 'id' to 'person' to remain consistent with transcript\n",
    "profile = profile.rename(columns={'id':'person'})\n",
    "# Remove people was ages > 99\n",
    "profile = profile[profile['age'] <= 99]\n",
    "\n",
    "# Remove people with income > 1000000\n",
    "profile = profile[profile['income'] < 1000000.0]\n",
    "\n",
    "# Binarise age based on criteria old=1, young=0\n",
    "def age_group(age):\n",
    "    if age < 25:\n",
    "        return '< 25 years'\n",
    "    \n",
    "    if age < 35:\n",
    "        return '25 - 35 years'\n",
    "    \n",
    "    if age < 45:\n",
    "        return '36 - 45 years'\n",
    "    \n",
    "    if age < 66: \n",
    "        return '46 - 66 years'\n",
    "    \n",
    "    else: \n",
    "        return '67+ years'\n",
    "\n",
    "profile['age_group'] = profile['age'].apply(lambda x:age_group(x))\n",
    "\n",
    "# Binarise income based on criteria rich=1, poor=0\n",
    "def income_group(income):\n",
    "    if income < 50001: \n",
    "        return 'low income'\n",
    "    if income < 70001:\n",
    "        return 'med - low income'\n",
    "    if income < 90001:\n",
    "        return 'high - med income'\n",
    "    else:\n",
    "        return 'high income'\n",
    "\n",
    "profile['income_group'] = profile['income'].apply(lambda x:income_group(x))\n",
    "\n",
    "# select columns needed for training model\n",
    "profile_subset = profile[['person','income_group','age_group', 'gender']]\n",
    "\n",
    "# reset index to allow ease concatanation with transformed ohe data\n",
    "profile_subset = profile_subset.reset_index()\n",
    "\n",
    "profile_subset.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a35fcd34-8829-4c80-b86e-8fa82c0fdb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile['income'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd21b16-7811-4e19-a2a9-667623d6aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "offer_per_person_demo = offer_df.merge(profile_subset, left_on='person',right_on='person', how='left').set_index('person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7771dfa-99d4-4667-b8a5-6e4ae35d99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = offer_per_person_demo.drop(['bogo', 'discount'], axis=1)\n",
    "y = offer_per_person_demo[['bogo', 'discount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20de2ef3-e30e-445f-b776-d07e671382d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13009 entries, 0009655768c64bdeb2e877511632db8f to ffff82501cea40309d5fdd7edcca4a07\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   income_group  11349 non-null  object\n",
      " 1   age_group     11349 non-null  object\n",
      " 2   gender        11349 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 406.5+ KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e39e24f-b0f1-48ed-b154-d2555f846056",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2dc4cb8-2a8f-4d41-8c10-c5864fa96079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intantiate OHE\n",
    "categorical_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "# List categorical variables in to be OHE\n",
    "categorical_columns = ['income_group','age_group','gender']\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "        [\n",
    "            ('cat', categorical_encoder, categorical_columns),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24251d8d-280c-49fa-bfa2-9610168858f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "clf = MultiOutputClassifier(RandomForestClassifier())\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('preprocess', preprocessing),\n",
    "        ('classifier', clf),\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43509265-560a-4a1d-ad5b-8a425fcd43aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluating classifier using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "098e56b7-be2a-4899-9fe8-768772af0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_prof_per_cust(y_test, y_pred):\n",
    "    \n",
    "    '''\n",
    "    This function returns the expected profit per user, \n",
    "    which is used as a the evaluation metric of the classifier.\n",
    "    '''\n",
    "    \n",
    "    # Defining benefits associated with true positive, true negatives\n",
    "    b_tp = 10\n",
    "    b_tn = 0\n",
    "\n",
    "    # Defining costs (or negative benefits) associated with false postives, false negatives\n",
    "    b_fp = -1\n",
    "    b_fn = -10 \n",
    "\n",
    "    # Calculating probabilities needed to compute expected profit \n",
    "\n",
    "    p_bogo = y_test['bogo'].value_counts().sort_index(ascending=True)[1]\n",
    "    n_bogo = y_test['bogo'].value_counts().sort_index(ascending=True)[0]\n",
    "\n",
    "    p_disc = y_test['discount'].value_counts().sort_index(ascending=True)[1]\n",
    "    n_disc = y_test['discount'].value_counts().sort_index(ascending=True)[0]\n",
    "\n",
    "    prob_p_bogo = p_bogo/(p_bogo + n_bogo)\n",
    "    prob_n_bogo = n_bogo/(p_bogo + n_bogo) \n",
    "\n",
    "    prob_p_disc = p_disc/(p_disc + n_disc)\n",
    "    prob_n_disc = n_disc/(p_disc + n_disc)\n",
    "\n",
    "    # Extracting values from confusion matrix \n",
    "\n",
    "    tn_bogo, fp_bogo, fn_bogo, tp_bogo = multilabel_confusion_matrix(y_test, y_pred)[0].ravel()\n",
    "    tn_disc, fp_disc, fn_disc, tp_disc = multilabel_confusion_matrix(y_test, y_pred)[1].ravel()\n",
    "     \n",
    "    p_tp_bogo = tp_bogo/p_bogo\n",
    "    p_tn_bogo = tn_bogo/n_bogo \n",
    "    \n",
    "    p_tp_disc = tp_disc/p_disc\n",
    "    p_tn_disc = tn_disc/n_disc \n",
    "\n",
    "    p_fp_bogo = fp_bogo/n_bogo\n",
    "    p_fn_bogo = fn_bogo/p_bogo\n",
    "\n",
    "    p_fp_disc = fp_disc/n_disc\n",
    "    p_fn_disc = fn_disc/p_disc\n",
    "    \n",
    "\n",
    "     # Expected profit\n",
    "    E_prof_bogo = (prob_p_bogo * (p_tp_bogo * b_tp + p_fn_bogo * b_fn)) + (prob_n_bogo * (p_tn_bogo * b_tn + p_fp_bogo * b_fp))\n",
    "                                                    \n",
    "    E_prof_disc = (prob_p_disc * (p_tp_disc * b_tp + p_fn_disc * b_fn)) + (prob_n_disc * (p_tn_disc * b_tn + p_fp_disc * b_fp))\n",
    "\n",
    "   \n",
    "    E_prof = E_prof_bogo + E_prof_disc\n",
    "    #E_prof = E_prof_bogo\n",
    "\n",
    "    \n",
    "    return E_prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "075d790e-1506-4b32-856f-a912adaf3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use crossval to eval performance of model before hypertuning\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "class_score = make_scorer(exp_prof_per_cust)\n",
    "\n",
    "cv_results = cross_validate(pipeline, X, y, cv = 5, scoring = class_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f25ab5e-b658-491e-b13a-ed66491191f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.330880749761887"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c342b77-aa27-4283-a15e-6af57ab37f7b",
   "metadata": {},
   "source": [
    "### Tuning classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40d839cc-eeb6-4b81-8162-608eb2b687a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__estimator__criterion': 'entropy',\n",
       " 'classifier__estimator__n_estimators': 50}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, make_scorer\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class_score = make_scorer(exp_prof_per_cust)\n",
    "\n",
    "\n",
    "parameters = {'classifier__estimator__n_estimators': [50, 100, 150], 'classifier__estimator__criterion':('gini', 'entropy')}\n",
    "\n",
    "rs = GridSearchCV(pipeline, param_grid=parameters, cv=3, scoring=class_score)\n",
    "rs.fit(X,y)\n",
    "\n",
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e07394f-120e-4d83-a1e9-a4fbf3b2c445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.54882987, 1.12302828, 1.76385474, 0.58643174, 1.04682239,\n",
       "        1.55955871]),\n",
       " 'std_fit_time': array([0.01515783, 0.0354294 , 0.01790023, 0.0736628 , 0.00938373,\n",
       "        0.02348126]),\n",
       " 'mean_score_time': array([0.05415042, 0.11164077, 0.16065399, 0.05494531, 0.09629631,\n",
       "        0.13540061]),\n",
       " 'std_score_time': array([0.00045534, 0.0248401 , 0.01871889, 0.00122575, 0.00311176,\n",
       "        0.00237964]),\n",
       " 'param_classifier__estimator__criterion': masked_array(data=['gini', 'gini', 'gini', 'entropy', 'entropy',\n",
       "                    'entropy'],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__estimator__n_estimators': masked_array(data=[50, 100, 150, 50, 100, 150],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier__estimator__criterion': 'gini',\n",
       "   'classifier__estimator__n_estimators': 50},\n",
       "  {'classifier__estimator__criterion': 'gini',\n",
       "   'classifier__estimator__n_estimators': 100},\n",
       "  {'classifier__estimator__criterion': 'gini',\n",
       "   'classifier__estimator__n_estimators': 150},\n",
       "  {'classifier__estimator__criterion': 'entropy',\n",
       "   'classifier__estimator__n_estimators': 50},\n",
       "  {'classifier__estimator__criterion': 'entropy',\n",
       "   'classifier__estimator__n_estimators': 100},\n",
       "  {'classifier__estimator__criterion': 'entropy',\n",
       "   'classifier__estimator__n_estimators': 150}],\n",
       " 'split0_test_score': array([4.48074706, 4.5642149 , 4.46368457, 4.46852663, 4.20728614,\n",
       "        4.21627853]),\n",
       " 'split1_test_score': array([4.44810886, 4.3597786 , 4.26314576, 4.3150369 , 4.06019373,\n",
       "        4.26752768]),\n",
       " 'split2_test_score': array([4.59640221, 4.58740775, 4.59640221, 4.75645756, 4.57380074,\n",
       "        4.57380074]),\n",
       " 'mean_test_score': array([4.50841938, 4.50380041, 4.44107752, 4.51334037, 4.28042687,\n",
       "        4.35253565]),\n",
       " 'std_test_score': array([0.06362415, 0.10227802, 0.13698729, 0.18297405, 0.21596329,\n",
       "        0.15785077]),\n",
       " 'rank_test_score': array([2, 3, 4, 1, 6, 5], dtype=int32)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ec768-dea2-49e1-964d-ef11e86322c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
